{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Grade 1  Grade 2  Grade 3  Grade 4  Grade 5  NaN\n",
      "0        0        0        0        1        0    0\n",
      "1        0        1        0        0        0    0\n",
      "2        1        0        0        0        0    0\n",
      "3        0        0        0        0        1    0\n",
      "4        0        0        1        0        0    0\n",
      "5        0        0        0        0        1    0\n",
      "6        0        1        0        0        0    0\n",
      "7        0        0        0        0        1    0\n",
      "8        1        0        0        0        0    0\n",
      "9        0        0        1        0        0    0\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "    0.0   1.0  NaN \n",
      "0     1     0     0\n",
      "1     1     0     0\n",
      "2     1     0     0\n",
      "3     1     0     0\n",
      "4     1     0     0\n",
      "5     1     0     0\n",
      "6     0     1     0\n",
      "7     1     0     0\n",
      "8     1     0     0\n",
      "9     1     0     0\n",
      "    7.0    9.0    10.0   11.0   12.0   13.0   20.0   21.0   22.0   23.0  \\\n",
      "0      0      0      0      0      0      0      0      0      0      0   \n",
      "1      0      0      0      0      0      0      0      0      0      0   \n",
      "2      0      0      0      0      0      0      0      0      0      0   \n",
      "3      0      0      0      0      0      0      0      0      0      0   \n",
      "4      0      0      0      0      0      0      0      0      0      0   \n",
      "5      0      0      0      0      0      0      0      1      0      0   \n",
      "6      0      0      0      0      0      0      0      0      0      0   \n",
      "7      0      0      0      0      0      0      0      0      0      0   \n",
      "8      0      0      0      0      0      0      0      1      0      0   \n",
      "9      0      0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "   ...     39.0   40.0   43.0   44.0   45.0   46.0   47.0   48.0   51.0  NaN    \n",
      "0  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "1  ...        0      0      0      1      0      0      0      0      0      0  \n",
      "2  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "3  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "4  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "5  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "6  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "7  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "8  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "9  ...        1      0      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[10 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "columns = ['damage_grade']\n",
    "y = df.damage_grade\n",
    "y = pd.get_dummies(y, dummy_na=True)\n",
    "print(y[:10])\n",
    "columns = ['has_geotechnical_risk_fault_crack',\n",
    "          'has_geotechnical_risk_flood',\n",
    "          'has_geotechnical_risk_land_settlement',\n",
    "          'has_geotechnical_risk_landslide',\n",
    "          'has_geotechnical_risk_liquefaction',\n",
    "          'has_geotechnical_risk_other',\n",
    "          'has_geotechnical_risk_rock_fall'\n",
    "          ]\n",
    "x = pd.DataFrame(df, columns=columns)\n",
    "\n",
    "column_has_repair_started = []\n",
    "for i in df.has_repair_started:\n",
    "    if i == 0.0 or i == 1.0:\n",
    "        column_has_repair_started.append(int(i))\n",
    "    else:\n",
    "        column_has_repair_started.append(0)\n",
    "print(column_has_repair_started[:10])\n",
    "has_repair_started = pd.DataFrame(column_has_repair_started)\n",
    "x = pd.concat([x, has_repair_started],axis=1) \n",
    "x = x.rename(columns={0: 'has_repair_started'})\n",
    "\n",
    "column_has_geotechnical_risk = []\n",
    "for i in df.has_geotechnical_risk:\n",
    "    if i == 0.0 or i == 1.0:\n",
    "        column_has_geotechnical_risk.append(int(i))\n",
    "    else:\n",
    "        column_has_geotechnical_risk.append(0)\n",
    "print(column_has_geotechnical_risk[:10])\n",
    "has_geotechnical_risk = pd.DataFrame(column_has_geotechnical_risk)\n",
    "x = pd.concat([x, has_geotechnical_risk],axis=1) \n",
    "x = x.rename(columns={0: 'has_geotechnical_risk'})\n",
    "\n",
    "has_geotechnical_risk = df.has_geotechnical_risk\n",
    "has_geotechnical_risk = pd.get_dummies(has_geotechnical_risk, dummy_na=True)\n",
    "print(has_geotechnical_risk[:10])\n",
    "x = pd.concat([has_geotechnical_risk, x],axis=1)\n",
    "\n",
    "district_id = df.district_id\n",
    "district_id = pd.get_dummies(district_id, dummy_na=True)\n",
    "print(district_id[:10])\n",
    "x = pd.concat([district_id, x],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area_assesed                              object\n",
      "building_id                               object\n",
      "damage_grade                              object\n",
      "district_id                                int64\n",
      "has_geotechnical_risk                    float64\n",
      "has_geotechnical_risk_fault_crack          int64\n",
      "has_geotechnical_risk_flood                int64\n",
      "has_geotechnical_risk_land_settlement      int64\n",
      "has_geotechnical_risk_landslide            int64\n",
      "has_geotechnical_risk_liquefaction         int64\n",
      "has_geotechnical_risk_other                int64\n",
      "has_geotechnical_risk_rock_fall            int64\n",
      "has_repair_started                       float64\n",
      "vdcmun_id                                  int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(631761, 44)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 44)                1980      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 29)                1305      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 29)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 17)                510       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 6)                 108       \n",
      "=================================================================\n",
      "Total params: 3,903\n",
      "Trainable params: 3,903\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(44, input_dim=44, activation='relu'))\n",
    "model.add(Dense(29, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(17, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = x[:500000]\n",
    "train_labels = y[:500000]\n",
    "test_data = x[500000:]\n",
    "test_labels = y[500000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500000 samples, validate on 131761 samples\n",
      "Epoch 1/10\n",
      "500000/500000 [==============================] - 24s 47us/step - loss: 1.3501 - acc: 0.4185 - val_loss: 1.3248 - val_acc: 0.4320\n",
      "Epoch 2/10\n",
      "500000/500000 [==============================] - 22s 44us/step - loss: 1.3328 - acc: 0.4265 - val_loss: 1.3245 - val_acc: 0.4321\n",
      "Epoch 3/10\n",
      "500000/500000 [==============================] - 21s 41us/step - loss: 1.3306 - acc: 0.4281 - val_loss: 1.3226 - val_acc: 0.4324\n",
      "Epoch 4/10\n",
      "500000/500000 [==============================] - 21s 42us/step - loss: 1.3295 - acc: 0.4282 - val_loss: 1.3204 - val_acc: 0.4314\n",
      "Epoch 5/10\n",
      "500000/500000 [==============================] - 22s 44us/step - loss: 1.3285 - acc: 0.4283 - val_loss: 1.3200 - val_acc: 0.4312\n",
      "Epoch 6/10\n",
      "500000/500000 [==============================] - 21s 42us/step - loss: 1.3284 - acc: 0.4286 - val_loss: 1.3206 - val_acc: 0.4320\n",
      "Epoch 7/10\n",
      "500000/500000 [==============================] - 21s 43us/step - loss: 1.3280 - acc: 0.4293 - val_loss: 1.3206 - val_acc: 0.4329\n",
      "Epoch 8/10\n",
      "500000/500000 [==============================] - 20s 40us/step - loss: 1.3279 - acc: 0.4288 - val_loss: 1.3191 - val_acc: 0.4330\n",
      "Epoch 9/10\n",
      "500000/500000 [==============================] - 24s 48us/step - loss: 1.3272 - acc: 0.4288 - val_loss: 1.3196 - val_acc: 0.4321\n",
      "Epoch 10/10\n",
      "500000/500000 [==============================] - 23s 47us/step - loss: 1.3272 - acc: 0.4284 - val_loss: 1.3224 - val_acc: 0.4326\n",
      "Accuracy: 43.26%\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=10, batch_size=100)\n",
    "scores = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/test.csv')\n",
    "columns = ['has_geotechnical_risk_fault_crack',\n",
    "          'has_geotechnical_risk_flood',\n",
    "          'has_geotechnical_risk_land_settlement',\n",
    "          'has_geotechnical_risk_landslide',\n",
    "          'has_geotechnical_risk_liquefaction',\n",
    "          'has_geotechnical_risk_other',\n",
    "          'has_geotechnical_risk_rock_fall'\n",
    "          ]\n",
    "xnew = pd.DataFrame(df, columns=columns)\n",
    "\n",
    "column_has_repair_started = []\n",
    "for i in df.has_repair_started:\n",
    "    if i == 0.0 or i == 1.0:\n",
    "        column_has_repair_started.append(int(i))\n",
    "    else:\n",
    "        column_has_repair_started.append(0)\n",
    "has_repair_started = pd.DataFrame(column_has_repair_started)\n",
    "xnew = pd.concat([xnew, has_repair_started],axis=1) \n",
    "xnew = xnew.rename(columns={0: 'has_repair_started'})\n",
    "\n",
    "column_has_geotechnical_risk = []\n",
    "for i in df.has_geotechnical_risk:\n",
    "    if i == 0.0 or i == 1.0:\n",
    "        column_has_geotechnical_risk.append(int(i))\n",
    "    else:\n",
    "        column_has_geotechnical_risk.append(0)\n",
    "has_geotechnical_risk = pd.DataFrame(column_has_geotechnical_risk)\n",
    "xnew = pd.concat([has_geotechnical_risk, xnew],axis=1) \n",
    "xnew = xnew.rename(columns={0: 'has_geotechnical_risk'})\n",
    "\n",
    "has_geotechnical_risk = df.has_geotechnical_risk\n",
    "has_geotechnical_risk = pd.get_dummies(has_geotechnical_risk, dummy_na=True)\n",
    "xnew = pd.concat([has_geotechnical_risk, xnew],axis=1)\n",
    "\n",
    "district_id = df.district_id\n",
    "district_id = pd.get_dummies(district_id, dummy_na=True)\n",
    "xnew = pd.concat([district_id, xnew],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynew = model.predict(xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.49538964e-01   3.12375098e-01   3.64545494e-01   3.76503021e-01\n",
      "    8.56564522e-01   2.90691832e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]\n",
      " [  1.35415509e-01   1.66470096e-01   2.91241467e-01   3.84356439e-01\n",
      "    9.15508449e-01   3.85149634e-10]]\n"
     ]
    }
   ],
   "source": [
    "print(ynew[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
