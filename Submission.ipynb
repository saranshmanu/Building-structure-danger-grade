{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['damage_grade']\n",
    "y = df.damage_grade\n",
    "y = pd.get_dummies(y, dummy_na=False)\n",
    "print(y[:10])\n",
    "columns = ['has_geotechnical_risk_fault_crack',\n",
    "          'has_geotechnical_risk_flood',\n",
    "          'has_geotechnical_risk_land_settlement',\n",
    "          'has_geotechnical_risk_landslide',\n",
    "          'has_geotechnical_risk_liquefaction',\n",
    "          'has_geotechnical_risk_other',\n",
    "          'has_geotechnical_risk_rock_fall'\n",
    "          ]\n",
    "x = pd.DataFrame(df, columns=columns)\n",
    "\n",
    "column_has_repair_started = []\n",
    "for i in df.has_repair_started:\n",
    "    if i == 0.0 or i == 1.0:\n",
    "        column_has_repair_started.append(int(i))\n",
    "    else:\n",
    "        column_has_repair_started.append(0)\n",
    "print(column_has_repair_started[:10])\n",
    "has_repair_started = pd.DataFrame(column_has_repair_started)\n",
    "x = pd.concat([x, has_repair_started],axis=1) \n",
    "x = x.rename(columns={0: 'has_repair_started'})\n",
    "\n",
    "column_has_geotechnical_risk = []\n",
    "for i in df.has_geotechnical_risk:\n",
    "    if i == 0.0 or i == 1.0:\n",
    "        column_has_geotechnical_risk.append(int(i))\n",
    "    else:\n",
    "        column_has_geotechnical_risk.append(0)\n",
    "print(column_has_geotechnical_risk[:10])\n",
    "has_geotechnical_risk = pd.DataFrame(column_has_geotechnical_risk)\n",
    "x = pd.concat([x, has_geotechnical_risk],axis=1) \n",
    "x = x.rename(columns={0: 'has_geotechnical_risk'})\n",
    "\n",
    "has_geotechnical_risk = df.has_geotechnical_risk\n",
    "has_geotechnical_risk = pd.get_dummies(has_geotechnical_risk, dummy_na=False)\n",
    "print(has_geotechnical_risk[:10])\n",
    "x = pd.concat([has_geotechnical_risk, x],axis=1)\n",
    "\n",
    "district_id = df.district_id\n",
    "district_id = pd.get_dummies(district_id, dummy_na=False)\n",
    "print(district_id[:10])\n",
    "x = pd.concat([district_id, x],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(44, input_dim=42, activation='linear'))\n",
    "model.add(Dense(29, activation='linear'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(17, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = x[:500000]\n",
    "train_labels = y[:500000]\n",
    "test_data = x[500000:]\n",
    "test_labels = y[500000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=100, batch_size=10000)\n",
    "scores = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/test.csv')\n",
    "print(df.shape)\n",
    "columns = ['has_geotechnical_risk_fault_crack',\n",
    "          'has_geotechnical_risk_flood',\n",
    "          'has_geotechnical_risk_land_settlement',\n",
    "          'has_geotechnical_risk_landslide',\n",
    "          'has_geotechnical_risk_liquefaction',\n",
    "          'has_geotechnical_risk_other',\n",
    "          'has_geotechnical_risk_rock_fall'\n",
    "          ]\n",
    "xnew = pd.DataFrame(df, columns=columns)\n",
    "\n",
    "column_has_repair_started = []\n",
    "for i in df.has_repair_started:\n",
    "    if i == 0.0 or i == 1.0:\n",
    "        column_has_repair_started.append(int(i))\n",
    "    else:\n",
    "        column_has_repair_started.append(0)\n",
    "has_repair_started = pd.DataFrame(column_has_repair_started)\n",
    "xnew = pd.concat([xnew, has_repair_started],axis=1) \n",
    "xnew = xnew.rename(columns={0: 'has_repair_started'})\n",
    "\n",
    "column_has_geotechnical_risk = []\n",
    "for i in df.has_geotechnical_risk:\n",
    "    if i == 0.0 or i == 1.0:\n",
    "        column_has_geotechnical_risk.append(int(i))\n",
    "    else:\n",
    "        column_has_geotechnical_risk.append(0)\n",
    "has_geotechnical_risk = pd.DataFrame(column_has_geotechnical_risk)\n",
    "xnew = pd.concat([has_geotechnical_risk, xnew],axis=1) \n",
    "xnew = xnew.rename(columns={0: 'has_geotechnical_risk'})\n",
    "\n",
    "has_geotechnical_risk = df.has_geotechnical_risk\n",
    "has_geotechnical_risk = pd.get_dummies(has_geotechnical_risk, dummy_na=False)\n",
    "xnew = pd.concat([has_geotechnical_risk, xnew],axis=1)\n",
    "\n",
    "district_id = df.district_id\n",
    "district_id = pd.get_dummies(district_id, dummy_na=False)\n",
    "xnew = pd.concat([district_id, xnew],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynew = model.predict(xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = []\n",
    "for i in ynew:\n",
    "    ind = np.unravel_index(np.argmax(i, axis=None), i.shape)\n",
    "    if ind == (0, ):\n",
    "        grade.append('Grade 1')\n",
    "    elif ind == (1, ):\n",
    "        grade.append('Grade 2')\n",
    "    elif ind == (2, ):\n",
    "        grade.append('Grade 3')\n",
    "    elif ind == (3, ):\n",
    "        grade.append('Grade 4')\n",
    "    elif ind == (4, ):\n",
    "        grade.append('Grade 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_id = df.building_id\n",
    "grade_id = pd.DataFrame(grade)\n",
    "result = pd.concat([building_id, grade_id],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.rename(columns={0: 'damage_grade'})\n",
    "print(result[:10])\n",
    "result.to_csv('submission.csv',mode = 'w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Building_Ownership_Use = pd.read_csv('Dataset/Building_Ownership_Use.csv')\n",
    "Building_Structure = pd.read_csv('Dataset/Building_Structure.csv')\n",
    "Building_Structure.drop(Building_Structure.columns[0], axis=1, inplace=True)\n",
    "record = pd.concat([Building_Ownership_Use, Building_Structure],axis=1)\n",
    "masterReco = record\n",
    "land_surface_condition = pd.get_dummies(masterReco.land_surface_condition, dummy_na=False)\n",
    "foundation_type = pd.get_dummies(masterReco.foundation_type, dummy_na=False)\n",
    "roof_type = pd.get_dummies(masterReco.roof_type, dummy_na=False)\n",
    "ground_floor_type = pd.get_dummies(masterReco.ground_floor_type, dummy_na=False)\n",
    "other_floor_type = pd.get_dummies(masterReco.other_floor_type, dummy_na=False)\n",
    "position = pd.get_dummies(masterReco.position, dummy_na=False)\n",
    "plan_configuration = pd.get_dummies(masterReco.plan_configuration, dummy_na=False)\n",
    "legal_ownership_status = pd.get_dummies(masterReco.legal_ownership_status, dummy_na=False)\n",
    "condition_post_eq = pd.get_dummies(masterReco.condition_post_eq, dummy_na=False)\n",
    "col = [\n",
    "    land_surface_condition,\n",
    "    foundation_type,\n",
    "    roof_type,\n",
    "    ground_floor_type,\n",
    "    other_floor_type,\n",
    "    position,\n",
    "    plan_configuration,\n",
    "    legal_ownership_status,\n",
    "    condition_post_eq\n",
    "      ]\n",
    "m = pd.concat(col,axis=1) \n",
    "building_id = masterReco.building_id\n",
    "columns_drop = [\n",
    "    \"land_surface_condition\",\n",
    "    \"foundation_type\",\n",
    "    \"roof_type\",\n",
    "    \"ground_floor_type\",\n",
    "    \"other_floor_type\",\n",
    "    \"position\",\n",
    "    \"plan_configuration\",\n",
    "    \"legal_ownership_status\",\n",
    "    \"condition_post_eq\",\n",
    "    \"district_id\",\n",
    "    \"vdcmun_id\",\n",
    "    \"ward_id\",\n",
    "    \"building_id\"\n",
    "      ]\n",
    "masterReco.drop(columns_drop, axis=1, inplace=True)\n",
    "masterReco = pd.concat([masterReco, m],axis=1) \n",
    "masterReco = masterReco.astype(float)\n",
    "masterReco = pd.concat([building_id, masterReco],axis=1)\n",
    "masterReco.set_index(\"building_id\", inplace=True)\n",
    "masterReco.to_csv('master_reco.csv')\n",
    "print(masterReco.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/train.csv')\n",
    "flag = masterReco.loc[[df.building_id[0]]]\n",
    "for i in range(1, len(df)):\n",
    "    temp = masterReco.loc[[df.building_id[i]]]\n",
    "    flag = pd.concat([flag,temp],axis=0)\n",
    "    if i%10000==0:\n",
    "        print(int(i*100/len(df)), '% records')\n",
    "print('Completed finding all the results')\n",
    "flag = flag.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_record = pd.concat([df, flag], axis=1)\n",
    "saved_record.to_csv('advanced_train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_record.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('advanced_train_set.csv')\n",
    "columns = ['damage_grade']\n",
    "y = df.damage_grade\n",
    "y = pd.get_dummies(y, dummy_na=False)\n",
    "\n",
    "district_id = df.district_id\n",
    "district_id = pd.get_dummies(district_id, dummy_na=False)\n",
    "df = pd.concat([df, district_id],axis=1)\n",
    "area_assesed = df.area_assesed\n",
    "area_assesed = pd.get_dummies(area_assesed, dummy_na=False)\n",
    "df = pd.concat([df, area_assesed],axis=1)\n",
    "df = df.drop(['building_id', 'damage_grade', 'building_id.1', 'district_id', 'area_assesed', 'vdcmun_id', 'Unnamed: 0'], axis = 1)\n",
    "df = df.astype(float)\n",
    "df = df.fillna(0)\n",
    "x = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model = Sequential()\n",
    "model.add(Dense(120, input_dim=120, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(40, activation='sigmoid'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = x[:500000]\n",
    "train_labels = y[:500000]\n",
    "test_data = x[500000:]\n",
    "test_labels = y[500000:]\n",
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=10, batch_size=100)\n",
    "scores = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "model.save('advanced_trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Dataset/test.csv')\n",
    "flag_test = masterReco.loc[[df_test.building_id[0]]]\n",
    "for i in range(1, len(df_test)):\n",
    "    temp_test = masterReco.loc[[df_test.building_id[i]]]\n",
    "    flag_test = pd.concat([flag_test,temp_test],axis=0)\n",
    "    if i%10000==0:\n",
    "        print(int(i*100/len(df_test)), '% records')\n",
    "print('Completed finding all the results')\n",
    "flag_test = flag.reset_index()\n",
    "saved_record_test = pd.concat([df_test, flag_test], axis=1)\n",
    "saved_record_test.to_csv('advanced_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('advanced_test_set.csv')\n",
    "columns = ['damage_grade']\n",
    "district_id = df_test.district_id\n",
    "district_id = pd.get_dummies(district_id, dummy_na=False)\n",
    "df_test = pd.concat([df_test, district_id],axis=1)\n",
    "area_assesed = df_test.area_assesed\n",
    "area_assesed = df_test.get_dummies(area_assesed, dummy_na=False)\n",
    "df_test = pd.concat([df_test, area_assesed],axis=1)\n",
    "df_test = df_test.drop(['building_id', 'damage_grade', 'building_id.1', 'district_id', 'area_assesed', 'vdcmun_id', 'Unnamed: 0'], axis = 1)\n",
    "df_test = df_test.astype(float)\n",
    "df_test = df_test.fillna(0)\n",
    "xnew_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('advanced_trained_model.best.h5')\n",
    "ynew_test = model.predict(xnew_test)\n",
    "grade = []\n",
    "for i in ynew_test:\n",
    "    ind = np.unravel_index(np.argmax(i, axis=None), i.shape)\n",
    "    if ind == (0, ):\n",
    "        grade.append('Grade 1')\n",
    "    elif ind == (1, ):\n",
    "        grade.append('Grade 2')\n",
    "    elif ind == (2, ):\n",
    "        grade.append('Grade 3')\n",
    "    elif ind == (3, ):\n",
    "        grade.append('Grade 4')\n",
    "    elif ind == (4, ):\n",
    "        grade.append('Grade 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grade))\n",
    "building_id = df.building_id\n",
    "grade_id = pd.DataFrame(grade)\n",
    "result = pd.concat([building_id, grade_id],axis=1) \n",
    "result = result.rename(columns={0: 'damage_grade'})\n",
    "print(result[:10])\n",
    "result.to_csv('submission_advanced.csv',mode = 'w', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
